# ğŸš¢ Titanic - Predicting Survival with Machine Learning

Welcome aboard!  
This project is a hands-on machine learning exploration using the famous Titanic dataset from Kaggle.  
Itâ€™s basically the â€œHello, World!â€ of data science â€” and here I am, diving in.

---

## ğŸ¯ What I'm Trying to Do

- Explore the dataset (EDA)
- Make sense of the features and do some feature engineering
- Train machine learning models (coming soon!)
- Create and submit predictions to Kaggle
- Learn and have fun while building my data science workflow

---

## ğŸ—‚ï¸ Folder Structure (Work in Progress)

titanic-survival-prediction/
â”‚
â”œâ”€â”€ data/                 
â”‚   â”œâ”€â”€ raw/             
â”‚   â””â”€â”€ processed/      
â”‚
â”œâ”€â”€ notebooks/           
â”‚   â”œâ”€â”€ 01_EDA.ipynb
â”‚   â”œâ”€â”€ 02_Feature_Engineering.ipynb
â”‚   â”œâ”€â”€ 03_Modeling.ipynb
â”‚   â””â”€â”€ 04_Final_Submission.ipynb
â”‚
â”œâ”€â”€ scripts/               
â”‚   â”œâ”€â”€ preprocess.py      
â”‚   â”œâ”€â”€ train.py           
â”‚   â””â”€â”€ utils.py          
â”‚
â”œâ”€â”€ models/               
â”‚
â”œâ”€â”€ submissions/           
â”‚   â””â”€â”€ submission_2025-04-25.csv
â”‚
â”œâ”€â”€ figures/               
â”œâ”€â”€ requirements.txt       
â”œâ”€â”€ README.md             
â””â”€â”€ .gitignore            

---

## ğŸ› ï¸ Tech Stack

- Python 3.10.1
- pandas & numpy
- matplotlib & seaborn
- scikit-learn
- xgboost (eventually)

---

## ğŸ“ Progress Log

> This section will be updated as the project grows!

- [ ] EDA
- [ ] Feature Engineering
- [ ] Modeling
- [ ] Kaggle Submission

---

## ğŸ“ˆ Project Workflow

Hereâ€™s how I plan to approach the Titanic survival prediction:

1. **Exploratory Data Analysis (EDA)**  
   - Understand the structure of the data  
   - Check for missing values and class imbalance  
   - Visualize feature relationships (e.g., survival vs. age, class, sex)

2. **Feature Engineering**  
   - Create new features like "FamilySize", "Title", and "IsAlone"  
   - Handle missing values (e.g., Age, Fare)  
   - Encode categorical features

3. **Model Building**  
   - Try different classifiers: Logistic Regression, Random Forest, XGBoost  
   - Tune hyperparameters using cross-validation  
   - Compare model performance (accuracy, F1-score)

4. **Prediction & Submission**  
   - Select best model  
   - Generate predictions for test set  
   - Submit to Kaggle and evaluate result

5. **Future Plans**  
   - Try ensembling (voting, stacking)  
   - Use pipelines for cleaner workflow  
   - Document the whole process in scripts


## ğŸ¤“ Notes to Self

- Keep code clean and modular
- Try different models and compare results
- Donâ€™t be afraid to mess up â€” itâ€™s all part of the game

---

## âœï¸ Author

Made with curiosity by [hojjang98](https://github.com/hojjang98)  
Feel free to clone, fork, or just take a peek ğŸ˜„